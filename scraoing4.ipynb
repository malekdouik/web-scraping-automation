{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from re import search\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from lxml import html\n",
    "import requests\n",
    "import configparser\n",
    "from configparser import ConfigParser\n",
    "import ast \n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <b>Selenium get-URLs </b><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNYTimes_Urls(search,nbe_article):\n",
    "\n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(\"https://www.nytimes.com/\")\n",
    "\n",
    "    # click loop button\n",
    "    button = WebDriverWait(driver, 40).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"app\"]/div[2]/div[2]/header/section[1]/div[1]/div[2]/button'))).click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"app\"]/div[2]/div/header/section[1]/div[1]/div[2]/div/form/div/input').send_keys(search,Keys.ENTER)\n",
    "\n",
    "    time.sleep(7)\n",
    "\n",
    "    i=0\n",
    "    while i<=4:\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[1]/div[2]/main/div/div[1]/div[2]/div/div/div[1]/div/div/button'))).click()\n",
    "        i+=1\n",
    "\n",
    "    dateRannge =''\n",
    "\n",
    "    if(dateRannge =='yesterday'):\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[1]/div[2]/main/div/div[1]/div[2]/div/div/div[1]/div/div/div/ul/li[2]/button'))).click()\n",
    "\n",
    "    if(dateRannge =='Past week'):\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[1]/div[2]/main/div/div[1]/div[2]/div/div/div[1]/div/div/div/ul/li[3]/button'))).click() \n",
    "\n",
    "    if(dateRannge =='Pas Month'):\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[1]/div[2]/main/div/div[1]/div[2]/div/div/div[1]/div/div/div/ul/li[4]/button'))).click()\n",
    "\n",
    "    if(dateRannge =='Pas year'):\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[1]/div[2]/main/div/div[1]/div[2]/div/div/div[1]/div/div/div/ul/li[5]/button'))).click() \n",
    "\n",
    "    if(dateRannge == 'Specific Dates'): \n",
    "        \n",
    "        #start_date='08/04/2020'\n",
    "        #end_date='08/22/2020'\n",
    "\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"site-content\"]/div/div[1]/div[2]/div/div/div[1]/div/div/div/ul/li[6]/button'))).click()\n",
    "\n",
    "        driver.find_element_by_xpath('//*[@id=\"startDate\"]').send_keys(start_date,Keys.ENTER)   \n",
    "        driver.find_element_by_xpath('//*[@id=\"endDate\"]').send_keys(end_date,Keys.ENTER)  \n",
    "\n",
    "    # click on button show more \n",
    "    i=0\n",
    "    while i<= nbe_article :\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"site-content\"]/div/div[2]/div[2]/div/button'))).click()\n",
    "        i+=10\n",
    "    \n",
    "    # get all <a href=''> \n",
    "    time.sleep(5)\n",
    "    elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "\n",
    "    # Put all links that they have  searchResultPosition in list\n",
    "    a=driver.current_url\n",
    "   \n",
    "    liste=[driver.current_url]\n",
    "    links = driver.find_elements_by_xpath(\"//a[contains(@href, 'searchResultPosition=')]\")\n",
    "    for link in links:\n",
    "        liste.append(link.get_attribute(\"href\"))\n",
    "    #liste=[]\n",
    "    #for elem in elems:\n",
    "        #if search(\"searchResultPosition=\", elem.get_attribute(\"href\")):\n",
    "            #liste.append(elem.get_attribute(\"href\"))\n",
    "\n",
    "    return liste\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "def getCBS_Urls(search,nbe_article):\n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(\"https://www.cbsnews.com/\")\n",
    "\n",
    "    # click loop button\n",
    "    button = WebDriverWait(driver, 60).until(EC.visibility_of_element_located((By.XPATH, '/html/body/header/div/nav/ul/li[7]/a'))).click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"site-nav__search-field\"]').send_keys(search,Keys.ENTER)\n",
    "\n",
    "   \n",
    "\n",
    "    #Hover shows button\n",
    "    #element_to_hover_over = driver.find_element_by_xpath('//*[@id=\"mantle_skin\"]/div[2]/div/div/div/div/div[1]/div[1]/ul/li[2]/a')\n",
    "    #hover = ActionChains(driver).move_to_element(element_to_hover_over)\n",
    "    #hover.perform()\n",
    "\n",
    "\n",
    "    dateRange =' bnbnb,nn,njn'\n",
    "    time.sleep(5)\n",
    "    if(dateRange =='60 Minutes'):\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"mantle_skin\"]/div[2]/div/div/div/div/div[1]/div[1]/ul/li[2]/div/ul/li[2]/a'))).click()\n",
    "\n",
    "    if(dateRange =='48 Hours'):\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"mantle_skin\"]/div[2]/div/div/div/div/div[1]/div[1]/ul/li[2]/div/ul/li[3]/a'))).click()\n",
    "\n",
    "    if(dateRange =='CBS This Morning'):\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"mantle_skin\"]/div[2]/div/div/div/div/div[1]/div[1]/ul/li[2]/div/ul/li[4]/a'))).click()\n",
    "\n",
    "    if(dateRange =='CBS Evening News'):\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"mantle_skin\"]/div[2]/div/div/div/div/div[1]/div[1]/ul/li[2]/div/ul/li[5]/a'))).click()        \n",
    "\n",
    "    if(dateRange =='Sunday Morning'):\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"mantle_skin\"]/div[2]/div/div/div/div/div[1]/div[1]/ul/li[2]/div/ul/li[6]/a'))).click() \n",
    "\n",
    "    # click on next page + get href tags\n",
    "    time.sleep(20)\n",
    "    i=10\n",
    "    liste=[driver.current_url]\n",
    "    while i<= nbe_article :\n",
    "        links = driver.find_elements_by_xpath(\"//a[contains(@href, 'www.cbsnews.com/news/')]\")\n",
    "        for link in links:\n",
    "            liste.append(link.get_attribute(\"href\"))\n",
    "\n",
    "        # Click next page\n",
    "        #button = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[4]/div[2]/div[2]/div/div/div/div/div[1]/div[3]/a'))).click()\n",
    "        time.sleep(2)\n",
    "        element = driver.find_element_by_xpath('//*[@id=\"mantle_skin\"]/div[2]/div/div/div/div/div[1]/div[3]/a/div')\n",
    "        driver.execute_script(\"arguments[0].click();\", element)\n",
    "        i+=10\n",
    "\n",
    " \n",
    "    return liste\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "def getFox_Urls(search,nbe_article): \n",
    "    print('getFox_Urls()')\n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(\"https://www.foxnews.com/\")\n",
    "\n",
    "    time.sleep(5)\n",
    "    # click loop button\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"wrapper\"]/header/div[2]/div/div[2]/div[1]/a'))).click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"wrapper\"]/header/div[4]/div[1]/div/div/form/fieldset/input[1]').send_keys(search,Keys.ENTER)\n",
    "\n",
    "    # Select count type = Article\n",
    "\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"wrapper\"]/div[2]/div[1]/div/div[2]/div[2]/button'))).click()\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"wrapper\"]/div[2]/div[1]/div/div[2]/div[2]/ul/li[1]/label'))).click()\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"wrapper\"]/div[2]/div[1]/div/div[1]/div[2]/div'))).click()\n",
    "\n",
    "    # click on button show more \n",
    "    i=0\n",
    "    while i<= nbe_article :\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"wrapper\"]/div[2]/div[2]/div/div[3]/div[2]'))).click()\n",
    "        i+=10\n",
    "\n",
    "    liste=[driver.current_url]\n",
    "    links = driver.find_elements_by_xpath(\"//a[contains(@href, 'https://www.foxnews.com/')]\")\n",
    "    for link in links:\n",
    "        liste.append(link.get_attribute(\"href\"))\n",
    "    \n",
    "    liste.pop(1) # it is url of the privacy-policy page \n",
    "    liste.pop(1) # it is url of terms-of-use page \n",
    "    return liste    \n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "def getFinancialTimes_Urls(search,nbe_article):  \n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(\"https://www.ft.com/\")\n",
    "    time.sleep(5)\n",
    "    # click loop button\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"site-navigation\"]/div[2]/div/div/div[1]/a[2]'))).click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"o-header-search-term-primary\"]').send_keys(search,Keys.ENTER)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #next page\n",
    "    i=0\n",
    "    while i<= nbe_article :\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"site-content\"]/div/div[4]/div/a[2]'))).click()\n",
    "        i+=25\n",
    "    \n",
    "    liste=[driver.current_url]\n",
    "    links = driver.find_elements_by_xpath(\"//a[contains(@href, 'https://www.ft.com/content/')]\")\n",
    "    for link in links:\n",
    "        liste.append(link.get_attribute(\"href\"))\n",
    "    \n",
    "    liste.pop(1) # it is url of the privacy-policy page \n",
    "    liste.pop(1) # it is url of terms-of-use page \n",
    "    return liste\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "def getCNN_Urls(search,nbe_article):  \n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(\"https://edition.cnn.com/\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # click loop button\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"header-nav-container\"]/div/div[1]/div/div[4]/button'))).click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"header-search-bar\"]').send_keys(search,Keys.ENTER)\n",
    "\n",
    "    liste=[driver.current_url]\n",
    "  \n",
    "    i = 0\n",
    "    while i<= nbe_article : \n",
    "        links = driver.find_elements_by_xpath(\"//a[contains(@href,'www.cnn.com')]\")\n",
    "        for link in links:\n",
    "            liste.append(link.get_attribute(\"href\"))\n",
    "            \n",
    "        time.sleep(2) \n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[5]/div[2]/div/div[2]/div[2]/div/div[5]/div/div[3]'))).click()\n",
    "        i+=10\n",
    " \n",
    "    return liste\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------      \n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "def getWashingtonpost_Urls(search,nbe_article):  \n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(\"https://www.washingtonpost.com/\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Click loop button\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"search-btn\"]'))).click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"search-field\"]').send_keys(search,Keys.ENTER)\n",
    "\n",
    "    time.sleep(5)\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"filterByContent\"]'))).click()\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Article\"]'))).click()\n",
    "\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"main-content\"]/div/div/div[1]/form[2]/div[3]/div/div[3]/button'))).click()\n",
    "\n",
    "\n",
    "    # next page\n",
    "    liste=[driver.current_url]\n",
    "    time.sleep(5)\n",
    "    \n",
    "    i = 0\n",
    "    while i<= nbe_article : \n",
    "        links = driver.find_elements_by_xpath(\"//a[contains(@href,'https://www.washingtonpost.com/')]\")\n",
    "        for link in links:\n",
    "            liste.append(link.get_attribute(\"href\"))\n",
    "        \n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"main-content\"]/div/div/div[2]/div/ul/li[13]/a'))).click()\n",
    "        i+=10\n",
    "    return liste  \n",
    "#--------------------------------------------------------------------------------------------------------------------------------------    \n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "def getABCNews_Urls(search,nbe_article):  \n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(\"https://abcnews.go.com/international\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Click loop button\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"search\"]/form/span/span[1]'))).click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"search\"]/form/span/span[2]/input[1]').send_keys(search,Keys.ENTER)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #select only articles\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"fitt-analytics\"]/div/main/div[2]/section/div/div[2]/div[2]/select[1]'))).click()\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"fitt-analytics\"]/div/main/div[2]/section[1]/div/div[2]/span[1]'))).click()\n",
    "\n",
    "    # next page\n",
    "    liste=[driver.current_url]\n",
    "    time.sleep(5) \n",
    "    i = 0\n",
    "    while i<= nbe_article : \n",
    "        links = driver.find_elements_by_xpath(\"//a[contains(@href,'https://abcnews.go.com/')]\")\n",
    "        for link in links:\n",
    "            liste.append(link.get_attribute(\"href\"))\n",
    "        \n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"fitt-analytics\"]/div/main/div[2]/section[1]/div/div[5]/a[2]'))).click()\n",
    "        i+=10        \n",
    "\n",
    "    return liste\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "def getWallStreet_Urls(search,nbe_article): \n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(\"https://www.wsj.com/\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Click loop button\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"root\"]/div/div/div/div[1]/header/div[2]/button'))).click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"searchInput\"]').send_keys(search,Keys.ENTER)\n",
    "\n",
    "   \n",
    "    liste=[driver.current_url]\n",
    "    time.sleep(10) \n",
    "    i = 0\n",
    "     # next page\n",
    "    while i<= nbe_article : \n",
    "        links = driver.find_elements_by_xpath(\"//a[contains(@href,'https://www.wsj.com/articles/')]\")\n",
    "        for link in links:\n",
    "            liste.append(link.get_attribute(\"href\"))   \n",
    "        \n",
    "        # next page\n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CLASS_NAME, 'next-page'))).click()\n",
    "        i+=10        \n",
    "\n",
    "    return liste\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "def getUSAToday_Urls(search,nbe_article): \n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(\"https://www.usatoday.com/\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Click loop button\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '/html/body/header/nav/div[2]/div[1]/a'))).click()\n",
    "    driver.find_element_by_xpath('/html/body/header/nav/div[2]/div[1]/form/input').send_keys(search,Keys.ENTER)\n",
    "\n",
    "    time.sleep(5)\n",
    "    # next page\n",
    "    liste=[driver.current_url]\n",
    " \n",
    "    i = 0\n",
    "    while i<= nbe_article : \n",
    "        links = driver.find_elements_by_xpath(\"//a[contains(@href, '/story/news/')]\")\n",
    "        for link in links:\n",
    "            liste.append(link.get_attribute(\"href\"))   \n",
    "       \n",
    "        button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '/html/body/main/div[1]/div[4]/a[5]'))).click()\n",
    "        i+=10        \n",
    "\n",
    "    return liste\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getWallStreet_Urls('trump',30)\n",
    "#getCNN_Urls('trump',30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <b>Write and read from file </b><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write in config file where we can configure scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_infile_configureScraping(url,loop_xpath,input_xpath,next_page_xpath,baliz=[],class_name=[],xpath=[]):\n",
    "    # write data in a file\n",
    "    a=\"[\"+url+\"]\"\n",
    "    with open(\"test1.ini\", \"a\") as file1:\n",
    "        file1.write(a)\n",
    "        file1.write(\"\\n\")\n",
    "        file1.write(\"url =\" + url+'\\n')\n",
    "        file1.write(\"baliz =\" + str(baliz)+'\\n') \n",
    "        file1.write(\"class_name =\" + str(class_name)+'\\n')\n",
    "        file1.write(\"xpath =\" + str(xpath)+'\\n')\n",
    "        file1.write(\"loop button xpath =\" + str(loop_xpath)+'\\n')\n",
    "        file1.write(\"input xpath =\" + str(input_xpath)+'\\n')\n",
    "        file1.write(\"next_page css selector =\" + str(next_page_xpath)+'\\n')\n",
    "        file1.write(\"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write in file where we can store urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_infile_urls(lien,url=[]):\n",
    "    with open(\"test2.ini\", \"a\") as file1:\n",
    "        # convert list of list into list \n",
    "        flat_list = []\n",
    "        for sublist in url:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "        \n",
    "        file1.write(\"\\n\")\n",
    "        file1.write(\"[\"+lien+\"]\")\n",
    "        file1.write(\"\\n\")\n",
    "        file1.write(\"recherhce = \")\n",
    "        file1.write(flat_list[0])\n",
    "        del flat_list[0]\n",
    "        file1.write(\"\\n\")\n",
    "        file1.write(\"url =\" + str(flat_list))\n",
    "        file1.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read params scraping from config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_function(url):\n",
    "    ## read file config\n",
    "    config = ConfigParser()\n",
    "    config.read('test1.ini') \n",
    "    \n",
    "    ### read entity\n",
    "    baliz= config[url]['baliz']\n",
    "    class_name=config[url]['class_name']\n",
    "    lien = config[url]['url']\n",
    "    xpath = config[url]['xpath']\n",
    "    loop_xpath = config[url]['loop button xpath']\n",
    "    input_xpath = config[url]['input xpath']\n",
    "    next_page = config[url]['next_page css selector']\n",
    "    \n",
    "  # Convert str to list\n",
    "    if(len(baliz)!=0 and len(class_name)!=0):\n",
    "        baliz=ast.literal_eval(baliz) \n",
    "        class_name=ast.literal_eval(class_name) \n",
    "    if(len(xpath)!=0):\n",
    "        xpath=ast.literal_eval(xpath)\n",
    "    \n",
    "    return lien,baliz,class_name,xpath,loop_xpath,input_xpath,next_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_infile_urls(lien,recherche):\n",
    "    config = ConfigParser()\n",
    "    config.read('test2.ini')\n",
    "    \n",
    "    searchlien= config[lien]['recherhce']\n",
    "    if recherche == searchlien :\n",
    "        urls= config[lien]['url']\n",
    "        # Convert str to list\n",
    "        urls=ast.literal_eval(urls) \n",
    "        return urls\n",
    "    else:\n",
    "        print('your search has not yet been carried out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <b>Web-Scraping </b><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I created 4 functions that do the web-scraping because when you create a single function there is a problem with the reteurn => \n",
    "# So 4 functions each corresponds to a possible scenario that the user can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_function1(lien,tags,class_name,list_authors,list_publish_date,list_text,list_top_image,list_summary,resultat,list_link):    \n",
    "    print('Scraping 1')\n",
    "    url = lien\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    #- Bfs4\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    # - Newspaper3K :\n",
    "    authors=article.authors\n",
    "    publish_date=article.publish_date\n",
    "    text=article.text\n",
    "    top_image=article.top_image\n",
    "    summary=article.summary\n",
    "\n",
    "    list_authors.append(article.authors)    \n",
    "    list_publish_date.append(publish_date)\n",
    "    list_text.append(text)\n",
    "    list_top_image.append(top_image)\n",
    "    list_summary.append(summary)\n",
    "    list_link.append(lien)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "    'Authors':list_authors,\n",
    "    'Publish_date':list_publish_date,\n",
    "    'Text':list_text,\n",
    "    'image':list_top_image,\n",
    "    'Summary':list_summary,\n",
    "    'Link':list_link \n",
    "    })\n",
    "        \n",
    "    # - Beautifulsoup\n",
    "    df1 = pd.DataFrame({'' : []})\n",
    "    if len(tags)!=0 and len(class_name)!=0:\n",
    "        resultat.append(url)\n",
    "        for j in zip(tags, class_name):\n",
    "        # resultat.append(soup.findAll(tags, {\"class\": class_name}))\n",
    "            for each in soup.findAll(tags, class_=class_name):\n",
    "                resultat.append([each.text.strip()])\n",
    "                if not(len(resultat) == 0):\n",
    "                    df1 = pd.DataFrame({'result_scraping':resultat})\n",
    "                        \n",
    "                        \n",
    "\n",
    "    return df,df1\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "def scraping_function2(lien,list_authors,list_publish_date,list_text,list_top_image,list_summary,list_link):  \n",
    "    print('Scraping 2')\n",
    "    url = lien\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # - Newspaper3K :\n",
    "    authors=article.authors\n",
    "    publish_date=article.publish_date\n",
    "    text=article.text\n",
    "    top_image=article.top_image\n",
    "    summary=article.summary\n",
    "    list_authors.append(authors)    \n",
    "    list_publish_date.append(publish_date)\n",
    "    list_text.append(text)\n",
    "    list_top_image.append(top_image)\n",
    "    list_summary.append(summary)\n",
    "    list_link.append(lien) \n",
    "    df = pd.DataFrame({\n",
    "    'Authors':list_authors,\n",
    "    'Publish_date':list_publish_date,\n",
    "    'Text':list_text,\n",
    "    'image':list_top_image,\n",
    "    'Summary':list_summary,\n",
    "    'Link':list_link   \n",
    "    })\n",
    "    return df\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "def scraping_function3(lien,tags,class_name,xpath,list_authors,list_publish_date,list_text,list_top_image,list_summary,resultat,resultat_xpaths,list_link):  \n",
    "    print('Scraping 3')\n",
    "    \n",
    "    url = lien\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    print(url)\n",
    "    article.parse()\n",
    "    #- Bfs4\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    # - Newspaper3K :\n",
    "    authors=article.authors\n",
    "    publish_date=article.publish_date\n",
    "    text=article.text\n",
    "    top_image=article.top_image\n",
    "    summary=article.summary\n",
    "\n",
    "    list_authors.append(article.authors)    \n",
    "    list_publish_date.append(publish_date)\n",
    "    list_text.append(text)\n",
    "    list_top_image.append(top_image)\n",
    "    list_summary.append(summary)\n",
    "    list_link.append(lien) \n",
    "    \n",
    "    # - Beautifulsoup\n",
    "    df1 = pd.DataFrame({'' : []})\n",
    "    if len(tags)!=0 and len(class_name)!=0:\n",
    "        resultat.append(lien)\n",
    "        for j in zip(tags, class_name):\n",
    "        # resultat.append(soup.findAll(tags, {\"class\": class_name}))\n",
    "            for each in soup.findAll(tags, class_=class_name):\n",
    "                resultat.append([each.text.strip()])\n",
    "\n",
    "                if not(len(resultat) == 0):\n",
    "                    df1 = pd.DataFrame({'result_scraping':resultat})\n",
    "   \n",
    "\n",
    "    # - Beautifulsoup + Xpath\n",
    "    r = requests.get(url)\n",
    "    doc = html.fromstring(r.content)\n",
    "    # convert lower case\n",
    "    xpath = [x.lower() for x in xpath]\n",
    "    df2 = pd.DataFrame({'' : []})\n",
    "    for k in xpath:\n",
    "        link = doc.xpath(k)  \n",
    "        for j in link:\n",
    "            resultat_xpaths.append(j.text)\n",
    "            #print(resultat_xpaths)\n",
    "            df2 = pd.DataFrame({'xpaths':resultat_xpaths })\n",
    "            \n",
    "    df = pd.DataFrame({\n",
    "    'Authors':list_authors,\n",
    "    'Publish_date':list_publish_date,\n",
    "    'Text':list_text,\n",
    "    'image':list_top_image,\n",
    "    'Summary':list_summary,\n",
    "    'Link':list_link    \n",
    "    })\n",
    "    \n",
    "\n",
    "    return df,df1,df2\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "def scraping_function4(lien,xpath,list_authors,list_publish_date,list_text,list_top_image,list_summary,list_link,resultat_xpaths):    \n",
    "    print('Scraping 4')\n",
    "\n",
    "    url = lien\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    print(url)\n",
    "    article.parse()\n",
    "    #- Bfs4\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    # - Newspaper3K :\n",
    "    authors=article.authors\n",
    "    publish_date=article.publish_date\n",
    "    text=article.text\n",
    "    top_image=article.top_image\n",
    "    summary=article.summary\n",
    "\n",
    "    list_authors.append(article.authors)    \n",
    "    list_publish_date.append(publish_date)\n",
    "    list_text.append(text)\n",
    "    list_top_image.append(top_image)\n",
    "    list_summary.append(summary)\n",
    "    list_link.append(lien) \n",
    "    \n",
    "\n",
    "    # - Beautifulsoup + Xpath\n",
    "    r = requests.get(url)\n",
    "    doc = html.fromstring(r.content)\n",
    "    # convert lower case\n",
    "    xpath = [x.lower() for x in xpath]\n",
    "    df2 = pd.DataFrame({'' : []})\n",
    "    for k in xpath:\n",
    "        link = doc.xpath(k)  \n",
    "        for j in link:\n",
    "            resultat_xpaths.append(j.text)\n",
    "            #print(resultat_xpaths)\n",
    "            df2 = pd.DataFrame({'xpaths':resultat_xpaths })\n",
    "            \n",
    "    df = pd.DataFrame({\n",
    "    'Authors':list_authors,\n",
    "    'Publish_date':list_publish_date,\n",
    "    'Text':list_text,\n",
    "    'image':list_top_image,\n",
    "    'Summary':list_summary,\n",
    "    'Link':list_link    \n",
    "    })\n",
    "    \n",
    "\n",
    "    return df,df2\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Configure_getUrlsSelenium(lien,loop_xpath,input_xpath,search,next_page_xpath,nbe_article): \n",
    "    options =webdriver.ChromeOptions()\n",
    "    options.headless=False\n",
    "    prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "    driver.get(lien)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Click loop button\n",
    "    button = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, loop_xpath))).click()\n",
    "    time.sleep(2)\n",
    "    driver.find_element_by_xpath(input_xpath).send_keys(search,Keys.ENTER)\n",
    "\n",
    "    time.sleep(10)\n",
    "    # next page\n",
    "    liste=[driver.current_url]\n",
    "           \n",
    "    i = 0\n",
    "    while i<= nbe_article : \n",
    "        links = driver.find_elements_by_xpath(\"//a[contains(@href, '\"+lien+\"')]\")\n",
    "        for link in links:\n",
    "            liste.append(link.get_attribute(\"href\"))   \n",
    "       \n",
    "        button = WebDriverWait(driver, 50).until(EC.visibility_of_element_located((By.CSS_SELECTOR, next_page_xpath))).click()\n",
    "\n",
    "        i+=10  \n",
    "    # drop duplicated\n",
    "    liste=list(dict.fromkeys(liste))\n",
    "    # get only the number desired by the user\n",
    "    liste=liste[:nbe_article]\n",
    "    \n",
    "    driver.close()\n",
    "    return liste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <b>FLASK </b><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [21/Sep/2020 10:43:58] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Sep/2020 10:43:59] \"\u001b[33mGET /style.css HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [21/Sep/2020 10:43:59] \"\u001b[33mGET /script.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [21/Sep/2020 10:43:59] \"\u001b[33mGET /script.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [21/Sep/2020 10:43:59] \"\u001b[33mGET /style.css HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [21/Sep/2020 10:44:01] \"\u001b[37mGET /getUrl HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [21/Sep/2020 10:44:01] \"\u001b[33mGET /style.css HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [21/Sep/2020 10:44:01] \"\u001b[33mGET /script.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [21/Sep/2020 10:44:01] \"\u001b[33mGET /script.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [21/Sep/2020 10:44:01] \"\u001b[33mGET /style.css HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.hindustantimes.com/\n",
      "['https://www.hindustantimes.com/', 'https://www.hindustantimes.com/world-news/un-council-members-urge-halt-to-myanmar-arakan-army-fighting/story-vkyeHbjz0yfL8GQ7SXYv1J.html', 'https://www.hindustantimes.com/world-news/tunisia-arrests-7-suspects-after-deadly-weekend-attack/story-V2WOCRngbfrYO3KJzaV5BO.html', 'https://www.hindustantimes.com/tennis/osaka-reaches-semis-then-withdraws-to-protest-racial-injustice/story-rU9Oq4kBpbPkDIjMRm29DN.html']\n",
      "Scraping 2\n",
      "Scraping 2\n",
      "Scraping 2\n",
      "Scraping 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Sep/2020 10:45:38] \"\u001b[37mPOST /getUrl HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Authors  \\\n",
      "0                                                 []   \n",
      "1  [Press Trust Of India, Posted Shankhyaneel Sar...   \n",
      "2            [Associated Press, Posted Niyati Singh]   \n",
      "3                                                 []   \n",
      "\n",
      "               Publish_date  \\\n",
      "0                       NaT   \n",
      "1 2020-09-12 05:44:34+05:30   \n",
      "2 2020-09-07 21:28:24+05:30   \n",
      "3 2020-08-27 07:46:09+05:30   \n",
      "\n",
      "                                                Text  \\\n",
      "0  Sep 21, 2020 06:16 IST\\n\\nFor India, the princ...   \n",
      "1  world\\n\\nUpdated: Sep 12, 2020 05:44 IST\\n\\nA ...   \n",
      "2  world\\n\\nUpdated: Sep 07, 2020 21:28 IST\\n\\nTu...   \n",
      "3  tennis\\n\\nUpdated: Aug 27, 2020 07:46 IST\\n\\nN...   \n",
      "\n",
      "                                               image Summary  \\\n",
      "0  https://www.hindustantimes.com/images/app-imag...           \n",
      "1  https://www.hindustantimes.com/rf/image_size_9...           \n",
      "2  https://www.hindustantimes.com/rf/image_size_9...           \n",
      "3  https://www.hindustantimes.com/rf/image_size_9...           \n",
      "\n",
      "                                                Link  \n",
      "0                    https://www.hindustantimes.com/  \n",
      "1  https://www.hindustantimes.com/world-news/un-c...  \n",
      "2  https://www.hindustantimes.com/world-news/tuni...  \n",
      "3  https://www.hindustantimes.com/tennis/osaka-re...  \n",
      "##################\n"
     ]
    }
   ],
   "source": [
    "import flask\n",
    "from flask import Flask,render_template,request,request, jsonify\n",
    "from flask_wtf import FlaskForm\n",
    "from wtforms import SelectField\n",
    "\n",
    "# This class so that we can display the content <select> <option> in getUrl.html from config file test1.ini\n",
    "class SearchForm(FlaskForm):\n",
    "    # read data stored in test1.ini\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"test1.ini\")\n",
    "    liste=config.sections()\n",
    "    test_url_selected = SelectField('Test Suite Name', choices=liste)\n",
    "\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/configureScraping')\n",
    "def goToConfigureScraping():\n",
    "    return render_template('configureScraping - Copie.html')\n",
    "\n",
    "app.config['SECRET_KEY'] = 'secretkey1234'\n",
    "@app.route('/getUrl')\n",
    "def goTogetUrl():\n",
    "    forms = SearchForm()\n",
    "    \n",
    "    return render_template('getUrl.html', forms=forms)\n",
    "\n",
    "@app.route('/getUrl', methods=['POST'])\n",
    "def getUrl():\n",
    "    liste=[]\n",
    "    lien = request.form['url']\n",
    "    print(lien)    \n",
    "    search = request.form['your_search']\n",
    "    nbe_article=int(request.form['nbe_article'])\n",
    "    \n",
    "     ##### get urls\n",
    "#     if (lien == 'https://www.nytimes.com/'):\n",
    "#         liste.append(getNYTimes_Urls(search,nbe_article))\n",
    "#         print('Entered in if 1 ')\n",
    "#         write_infile_urls(lien,liste)\n",
    "        \n",
    "#     if (lien == 'https://www.cbsnews.com/'):\n",
    "#         liste.append(getCBS_Urls(search,nbe_article))\n",
    "#         print('Entered in if 2 ')\n",
    "#         write_infile_urls(lien,liste)\n",
    "        \n",
    "#     if (lien == 'https://www.foxnews.com/'):\n",
    "#         liste.append(getFox_Urls(search,nbe_article))\n",
    "#         print('Entered in if 3 ')\n",
    "#         write_infile_urls(lien,liste)\n",
    "        \n",
    "#     if (lien == 'https://www.ft.com/'):\n",
    "#         liste.append(getFinancialTimes_Urls(search,nbe_article))\n",
    "#         print('Entered in if 4 ')\n",
    "#         write_infile_urls(lien,liste)  \n",
    "        \n",
    "#     if (lien == 'https://edition.cnn.com/'):\n",
    "#         liste.append(getCNN_Urls(search,nbe_article))\n",
    "#         print('Entered in if 5 ')\n",
    "#         write_infile_urls(lien,liste)\n",
    "        \n",
    "#     if (lien == 'https://www.washingtonpost.com/'):\n",
    "#         liste.append(getWashingtonpost_Urls(search,nbe_article))\n",
    "#         print('Entered in if 6 ')\n",
    "#         write_infile_urls(lien,liste)\n",
    "        \n",
    "#     if (lien == 'https://abcnews.go.com/international'):\n",
    "#         liste.append(getABCNews_Urls(search,nbe_article))\n",
    "#         print('Entered in if 7 ')\n",
    "#         write_infile_urls(lien,liste)\n",
    "        \n",
    "#     if (lien == 'https://www.wsj.com/'):\n",
    "#         liste.append(getWallStreet_Urls(search,nbe_article))\n",
    "#         print('Entered in if 8 ')\n",
    "#         write_infile_urls(lien,liste) \n",
    "        \n",
    "#     if (lien == 'https://www.usatoday.com/'):\n",
    "#         liste.append(getUSAToday_Urls(search,nbe_article))\n",
    "#         print('Entered in if 9 ')\n",
    "#         write_infile_urls(lien,liste)             \n",
    "#     else:\n",
    "#         print('Entred in else')\n",
    "    liste.append(Configure_getUrlsSelenium(lien,read_file_function(lien)[4],read_file_function(lien)[5],search,read_file_function(lien)[6],nbe_article))\n",
    "    liste=liste[:nbe_article]\n",
    "    write_infile_urls(lien,liste)\n",
    "        \n",
    "    \n",
    "        \n",
    "    ######## call Scraping functions \n",
    "    # 1.convert list of list into list \n",
    "    liste2 = []\n",
    "    for sublist in liste:\n",
    "        for item in sublist:\n",
    "            liste2.append(item)       \n",
    "    \n",
    "    read_file_function(lien)   \n",
    "    del liste2[0]\n",
    "    print(liste2)\n",
    "    \n",
    "    # Je n'avais pas d'autre choix sinon le scraping Ã©crasera data et return que le dernier article \n",
    "    \n",
    "    ## 1. lists For Newspaper3K\n",
    "    list_authors=[]\n",
    "    list_publish_date=[]\n",
    "    list_text=[]\n",
    "    list_top_image=[]\n",
    "    list_summary=[]\n",
    "    list_links=[]\n",
    "    #2. list for class_name and tags\n",
    "    resultat=[]\n",
    "    #3. list for xpath\n",
    "    resultat_xpaths=[]\n",
    "    #//\n",
    "    \n",
    "    for i in liste2:\n",
    "        if(len(read_file_function(lien)[1])!=0 and len(read_file_function(lien)[2])!=0  and len(read_file_function(lien)[3])!=0):\n",
    "            df=scraping_function3(i,read_file_function(lien)[1],read_file_function(lien)[2],read_file_function(lien)[3],list_authors,list_publish_date,list_text,list_top_image,list_summary,resultat,resultat_xpaths,list_links)[0]\n",
    "            df2=scraping_function3(i,read_file_function(lien)[1],read_file_function(lien)[2],read_file_function(lien)[3],list_authors,list_publish_date,list_text,list_top_image,list_summary,resultat,resultat_xpaths,list_links)[1]\n",
    "            df3=scraping_function3(i,read_file_function(lien)[1],read_file_function(lien)[2],read_file_function(lien)[3],list_authors,list_publish_date,list_text,list_top_image,list_summary,resultat,resultat_xpaths,list_links)[2]\n",
    "            \n",
    "            df=df.drop_duplicates(subset=['Text'], keep='first')\n",
    "            df3=df3.drop_duplicates(subset=['xpaths'], keep='first')\n",
    "            \n",
    "        if(len(read_file_function(lien)[1])!=0 and len(read_file_function(lien)[2])!=0 ):\n",
    "            df=scraping_function1(i,read_file_function(lien)[1],read_file_function(lien)[2],list_authors,list_publish_date,list_text,list_top_image,list_summary,resultat,list_links)[0]\n",
    "            df2=scraping_function1(i,read_file_function(lien)[1],read_file_function(lien)[2],list_authors,list_publish_date,list_text,list_top_image,list_summary,resultat,list_links)[1]\n",
    "            df=df.drop_duplicates(subset=['Text'], keep='first')\n",
    "            \n",
    "        if(len(read_file_function(lien)[3])!=0 ):\n",
    "            df=scraping_function4(i,read_file_function(lien)[3],list_authors,list_publish_date,list_text,list_top_image,list_summary,list_links,resultat_xpaths)[0]\n",
    "            df2=scraping_function4(i,read_file_function(lien)[3],list_authors,list_publish_date,list_text,list_top_image,list_summary,list_links,resultat_xpaths)[1]\n",
    "            df=df.drop_duplicates(subset=['Text'], keep='first')\n",
    "        else:\n",
    "            df=scraping_function2(i,list_authors,list_publish_date,list_text,list_top_image,list_summary,list_links)\n",
    "            df=df.drop_duplicates(subset=['Text'], keep='first')\n",
    "     \n",
    "    ######## //Scraping \n",
    "    print(df)\n",
    "    print('##################')\n",
    " \n",
    "    json = df.to_json() \n",
    "    return jsonify(json)\n",
    "       \n",
    "               \n",
    "\n",
    "@app.route('/configureWebToScraping',methods=['POST'])\n",
    "def configureWebToScraping():\n",
    "    \n",
    "    lien = request.form['url']\n",
    "    classe = request.form['class_name']\n",
    "    html_tag = request.form['html_tag']\n",
    "    xpath = request.form['x_path']\n",
    "    loop_xpath = request.form['loop_xpath']\n",
    "    input_xpath = request.form['input_xpath']\n",
    "    next_page_xpath =request.form['next_page_cssSelect']\n",
    "    \n",
    "    if request.form['action'] == 'Save':\n",
    "        #table_tag = request.form['table_tag']\n",
    "        # put stings in list \n",
    "        if html_tag !='': \n",
    "            html_tag=html_tag.split('/')\n",
    "        if classe !='' : \n",
    "            classe=classe.split('/')\n",
    "        if xpath!='' :\n",
    "            xpath=xpath.split(' ')\n",
    "\n",
    "        #table_tag=table_tag.split('/')\n",
    "        print(lien)\n",
    "        print(classe)\n",
    "        print(html_tag)\n",
    "        write_infile_configureScraping(lien,loop_xpath,input_xpath,next_page_xpath,html_tag,classe,xpath)\n",
    "        \n",
    "    if request.form['action'] == 'Test Configuration':    \n",
    "        Configure_getUrlsSelenium(lien,loop_xpath,input_xpath,'Trump',next_page_xpath,30)\n",
    "        msg='Your configuration is good click on save to save it in a configuration file'\n",
    "        return render_template('configureScraping - Copie.html',value=msg) \n",
    "        \n",
    "    return render_template('configureScraping - Copie.html') \n",
    "\n",
    "    #return jsonify(df)\n",
    "\n",
    "# Test the user's configuration to know if he chooses the right xpath ... \n",
    "@app.route('/TestUserConfiguration', methods=['POST','GET'])\n",
    "def TestUserConfiguration():\n",
    "\n",
    "    lien = request.form['url']\n",
    "    loop_xpath = request.form['loop_xpath']\n",
    "    input_xpath = request.form['input_xpath']\n",
    "    next_page_xpath =request.form['next_page_cssSelect']\n",
    "    print(next_page_xpath)\n",
    "    print(loop_xpath)\n",
    "    print(input_xpath)\n",
    "    \n",
    "    loop_xpath=loop_xpath.lower()\n",
    "    input_xpath=input_xpath.lower()\n",
    "    next_page_xpath=next_page_xpath.lower()\n",
    "    try:\n",
    "        Configure_getUrlsSelenium(lien,loop_xpath,input_xpath,'Trump',next_page_xpath,30)\n",
    "        msg=\"The configuration is correct click on save to save your configuration\"\n",
    "    except ElementClickInterceptedException:\n",
    "        msg=\"The configuration is not correct\"\n",
    "    \n",
    "    return render_template('configureScraping - Copie.html')    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    app.run()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
